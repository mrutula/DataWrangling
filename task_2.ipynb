{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Generate Sparse Representations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAME : MRUTULA SURESH\n",
    "\n",
    "ID: 28967828\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING THE LIBRARY\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from itertools import chain\n",
    "import itertools\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import *\n",
    "from __future__ import division\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path of the folders containing the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_input_dir = './txt_files'\n",
    "counts_input_dir = './sparse_files'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TokenizeRawData function converts the string to lower and tokenizes it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeRawData(file):\n",
    "    \"\"\"\n",
    "        This function tokenizes a raw text document.\n",
    "    \"\"\"\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\") \n",
    "    \n",
    "    raw_article = file.lower() # cover all words to lowercase\n",
    "    tokenised_article = tokenizer.tokenize(raw_article) # tokenize each text files\n",
    "    return ( tokenised_article) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opens each file and tokenizes them using a function call to tokenizeRawData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_txt = {}\n",
    "for filename in os.listdir(words_input_dir): \n",
    "    filename = os.path.join(words_input_dir, filename)\n",
    "    if os.path.isfile(filename) and filename.endswith(\".txt\"):\n",
    "        with open(filename, \"r\") as input_file:\n",
    "            #regex to extract the name of the file \n",
    "            name = re.findall(r'(\\w+)[.]',filename)\n",
    "            #using the name of the file as the key to a dictionary and the values are the tokenized words of the file\n",
    "            tokenized_txt[name[0]] = tokenizeRawData(input_file.read())\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the stop words from the file\n",
    "stopwords = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the keys that is the name of the file in a list\n",
    "key_list = tokenized_txt.keys()\n",
    "keys=list(key_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the stop words from the list of tokens and storing it back into the dictionary\n",
    "stopwordsSet = set(stopwords)\n",
    "for k in keys:\n",
    "    tokenized_txt[k] = [w for w in tokenized_txt[k] if w not in stopwordsSet]\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yeah', 139),\n",
       " ('um', 139),\n",
       " ('uh', 139),\n",
       " ('good', 138),\n",
       " ('make', 138),\n",
       " ('mm', 138),\n",
       " ('meeting', 137),\n",
       " ('design', 137),\n",
       " ('mm-hmm', 137),\n",
       " ('things', 136),\n",
       " ('thing', 135),\n",
       " ('remote', 135),\n",
       " ('control', 134),\n",
       " ('bit', 133),\n",
       " ('people', 133),\n",
       " ('work', 133),\n",
       " ('put', 132),\n",
       " ('kind', 130),\n",
       " ('idea', 129),\n",
       " ('time', 129),\n",
       " ('buttons', 127),\n",
       " ('lot', 126),\n",
       " ('gonna', 124),\n",
       " ('user', 124),\n",
       " ('button', 124)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# storing the values of the dictionary in a list\n",
    "words_2 = list(chain.from_iterable([set(value) for value in tokenized_txt.values()]))\n",
    "fd_2 = FreqDist(words_2)      #finding the document frequency of the words\n",
    "fd_2.most_common(25)          #printing the 25 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "MoreFreqWords = set([k for k, v in fd_2.items() if v > 132])     #storing the words having a document frequency more than 132 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bit',\n",
       " 'control',\n",
       " 'design',\n",
       " 'good',\n",
       " 'make',\n",
       " 'meeting',\n",
       " 'mm',\n",
       " 'mm-hmm',\n",
       " 'people',\n",
       " 'remote',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'uh',\n",
       " 'um',\n",
       " 'work',\n",
       " 'yeah'}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MoreFreqWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the most common words from the list of tokens \n",
    "for k in keys:\n",
    "    tokenized_txt[k] = [w for w in tokenized_txt[k] if w not in MoreFreqWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the value of the modified dictionary in a list \n",
    "list_word = set(list(chain.from_iterable([set(value) for value in tokenized_txt.values()])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_list = []\n",
    "for i in  list_word:\n",
    "    if len(i) > 2:            #removing the words having a length less than 3\n",
    "        new_list.append(i)     #appending the word in a list\n",
    "new_list = sorted(new_list)     #sorting the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the list of words in a dictionary along with its index\n",
    "new_dict = {}\n",
    "count = 0\n",
    "for n in new_list:\n",
    "    new_dict[n] = count\n",
    "    count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the vocab.txt file \n",
    "with open('./vocab.txt', 'w') as file:\n",
    "    for k, v in new_dict.items():\n",
    "        file.write(str(k) + ':'+ str(v) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task2.b\n",
    "topic_seg.txt: It contains the topic boundaries encoded in boolean vectors. For example, if a meeting transcript, \"ES2018d.txt\" contains 10 paragraphs in total after being preprocessed, and there are topic boundaries after the 2nd, 5th, and 7th paragraphs, the boolean vector must be \"ES2018d:0,1,0,0,1,0,1,0,0,1\". Every line in topic_seg.txt corresponds to one meeting transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = {}           #creating a dictionary\n",
    "for xfile in os.listdir(words_input_dir): \n",
    "    xfile = os.path.join(words_input_dir, xfile)             #opening each file\n",
    "    if os.path.isfile(xfile) and xfile.endswith('.txt'):\n",
    "        with open(xfile) as file:\n",
    "            file_list = []\n",
    "            name = re.findall(r'(\\w+)[.]',xfile)             #to obatin the name of the file\n",
    "             \n",
    "            for line in file:                                #reading each line of the file \n",
    "                if '*' not in line:                          # checking if * is not present in that file\n",
    "                    file_list.append(0)                      #append 0 if true\n",
    "                else:\n",
    "                    file_list[-1] = 1                        #else the last element of the list is set to 1\n",
    "            string  = ','.join([str(_string) for _string in file_list])     # concatinating the 1 s and 0s with ','\n",
    "            \n",
    "            name_list[name[0]] =string                        #storing it the file name as the key and the string of 1 and 0 as value\n",
    "                \n",
    "                        \n",
    "                        \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the topic_segs.txt file\n",
    "with open('./topic_segs.txt', 'w') as file:\n",
    "    for k, v in name_list.items():\n",
    "        file.write(str(k) + ':'+ str(v) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 2 c \n",
    "\n",
    "./sparse_files/*.txt : Each txt file in the \"sparse_files\" folder corresponds to one of the meeting transcripts in the \"txt_files\" folder, and they have the same file name.  For example, \"./sparse_files/ES2002a.txt\" corresponds to \"./txt_files/ES2002a.txt\". Each file in \"/sparse_files\" contains the sparse representations for all its paragraphs as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the vocab.txt file and storing its content as a dictionary\n",
    "vocab = {}\n",
    "with open('./vocab.txt') as f:\n",
    "    for line in f:\n",
    "        (key, val) = line.split(':')\n",
    "        vocab[key] = int(val.strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for xfile in os.listdir(words_input_dir):        #opening each file\n",
    "    xfile = os.path.join(words_input_dir, xfile)\n",
    "    if os.path.isfile(xfile) and xfile.endswith('.txt'):\n",
    "        with open(xfile) as file:\n",
    "            name = re.findall(r'(\\w+.txt)',xfile)   #name of the file\n",
    "            main_list = []                           #Main list of the entire file\n",
    "            for line in file:                         # obtaining the lines in a file\n",
    "                sub_list = []                         # sub lists of individual lines\n",
    "                temp_word = []                         # temporary list of the words in a particular line\n",
    "                list_word = line.split()               #splitting the line based on space and converting it into a list of words\n",
    "                list_word = [x.lower() for x in list_word]  #converting each word in the list to lower     \n",
    "                for word in list_word:                     #checking each word of the list\n",
    "                    if word not in temp_word and word in vocab: #seeing if the word isnt in temp list and is present in dictionary\n",
    "                        temp_word.append(word)                    #append it in temp list \n",
    "                        vocab_index = vocab[word]                  #getting the index of word\n",
    "                        vocab_count = list_word.count(word)          #getting the count of word\n",
    "                        \n",
    "                        string_word = ':'.join([str(vocab_index),str(vocab_count)])      #concating the index and count\n",
    "                        sub_list.append(string_word)                #storing in list\n",
    "                main_list.append(sub_list)\n",
    "            vocfile = os.path.join(counts_input_dir, name[0])            \n",
    "            with open(vocfile, 'w') as file:                     #writing the strings into a file\n",
    "                for l in main_list:\n",
    "                    string  = ','.join([str(_string) for _string in l]) \n",
    "                    #string_new = re.sub('\\n\\n','',string)\n",
    "                    #file.write(string)\n",
    "                    if len(string) > 0:\n",
    "                        file.write(string + '\\n')\n",
    "                        \n",
    "                    \n",
    "                          \n",
    "                   \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
